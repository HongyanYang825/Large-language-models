{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933726cc-d389-4106-8d21-d988dd79ddbd",
   "metadata": {},
   "source": [
    "## Table of Contents\r\n",
    "* [0. Import libraries and setup file path](#0)\r\n",
    "* [1. Retrieve Domain Knowledge](#1)\r\n",
    "    * [1.1. Extract domain document page content](#1.1)\r\n",
    "    * [1.2. Document chunking](#1.2)\r\n",
    "    \t* [1). Create chunks of same length from sentences](#1.2.1)\r\n",
    "    \t* [2). Join chunk of sentences to a single string](#1.2.2)\r\n",
    "    * [1.3. Chunk embedding](#1.3)\r\n",
    "    \t* [1). Import pre-trained sentence-transformers model](#1.3.1)\r\n",
    "    \t* [2). Save the domain knowledge with embeddings](#1.3.2)\r\n",
    "* [2. Prompt Augmentation](#2)\r\n",
    "    * [2.1. Semantic search top k relevent chunks](#2.1)\r\n",
    "* [3. Text Generation](#3)\r\n",
    "    * [3.1. Import pre-trained seq2seq model](#3.1)\r\n",
    "    * [3.2. Test the Model with Zero-Shot Inferencing](#3.2)\r\n",
    "* [4. Prompt Augmentation](#4)\r\n",
    "    * [4.1. Fine-tune the generation model for abstractive question answering](#4.1)\r\n",
    "        * [1). Preprocess the abstractive-qa dataset](#4.1.1)\r\n",
    "        * [2). Setup the LoRA model for Fine-Tuning](#4.1.2)\r\n",
    "    \t* [3). Train the LoRA Adapter](#4.1.3)\r\n",
    "    * [4.2. Prompt engineering a causal language model for abstractive question answering](#4.2)\r\n",
    "        * [1). Model selection and setup](#4.2.1)\r\n",
    "        * [2). Answer from causal model without prompt engineering](#4.2.2)\r\n",
    "    \t* [3). Answer from causal model with prompt engineering](#4.2.3)\r\n",
    "* [5. Model Evaluation](#5)\r\n",
    "    * [5.1. Evaluate the Model Quantitatively (with ROUGE Metric)](#5.1)\r\n",
    "        * [1). Create sample questions and human_baseline answers](#5.1.1)\r\n",
    "        * [2). Get answers from the causal language model](#5.1.2)\r\n",
    "    \t* [3). Get answers from fine-tuned seq2seq language model](#5.1.3)\r\n",
    "    * [5.2. Import the ROUGE and BLEU metric](#5.2)\r\n",
    "    * [5.3. Evaluate models performance](#5.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7215a5a-14dd-403c-8516-f21d9ec64903",
   "metadata": {},
   "source": [
    "## 0. Import libraries and setup file path<a id=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b9af1b-af9a-47f6-80ee-9f6c9ff882a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz \n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "DOMAIN_DOCUMENT_PATH = 'domain_document/'\n",
    "DOMAIN_KNOWLEDGE_PATH = 'domain_document/domain_knowledge/'\n",
    "CHUNK_SIZE = 10 \n",
    "INTERSECT_SIZE = 2\n",
    "\n",
    "# Load the English language model\n",
    "NLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a874799-fe9e-46ba-a3e4-75bfe0a89c44",
   "metadata": {},
   "source": [
    "## 1. Retrieve Domain Knowledge<a id=\"1\"></a>\n",
    "### 1.1. Extract domain document page content<a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d688e86-0735-47dd-8a98-cd905de3188c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(title):\n",
    "    '''\n",
    "    Extract document content and preprocess\n",
    "    '''\n",
    "    # Open the document with given name\n",
    "    path = DOMAIN_DOCUMENT_PATH + title\n",
    "    document = fitz.open(path)  \n",
    "    # Read the document and get content page by page\n",
    "    content_list = list()\n",
    "    for index, page in tqdm(enumerate(document)): \n",
    "        raw_content = page.get_text() \n",
    "        content = raw_content.replace(\"\\n\", \" \").strip()\n",
    "        content_list.append({'title': title,\n",
    "                             \"page\": (index + 1),  \n",
    "                             \"content\": content})\n",
    "    return content_list\n",
    "\n",
    "def get_all_document():\n",
    "    '''\n",
    "    Extract content of all documents in the domain knowledge folder\n",
    "    '''\n",
    "    # Get titles of all documents\n",
    "    path = DOMAIN_DOCUMENT_PATH\n",
    "    doc_list = [_ for _ in os.listdir(path) if os.path.isfile(os.path.join(path, _))]\n",
    "    # Extract content for each document\n",
    "    all_content_list = list()\n",
    "    for title in tqdm(doc_list):\n",
    "        all_content_list.extend(get_document(title))\n",
    "    return all_content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75710fed-8668-4ad7-a060-679c7303f5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab18835baf0e4736b216f5851eb6c741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c3c8173e4c4a499882eca6f4ce1fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bf795afb194c47a8e7e067dd8dd947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2ee0ed7860485c86f58e92824c9cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a273266994284e66a6f76f2b7418ab57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259deba14e648de906b589e42d72273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aed06cc2e394e8287b86af4c5e1c8c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6538f3c872204639835aab39b0b0eb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9710116a0704b0ca5011d2ac9cf533c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Enterprise-Analytics-Report-508-FINAL.pdf',\n",
       " 'page': 12,\n",
       " 'content': \"12    Leveraging Technologies to Analyze and Visualize  Data  The relationship between the CDO and CIO is also critical to the success of enterprise analytics  programs. The collaboration between these roles varies across agencies, with different models in  place such as reporting and non-reporting relationships. Regardless of the specific structure, the  interplay between the CDO and the CIO is crucial for effectively linking enterprise analytics  programs and IT strategies, thus ensuring the necessary technology and infrastructure are in  place to support the agency's enterprise analytics initiatives.  Table 2. Has your agency implemented an enterprise analytics platform for integrating and  analyzing data across various components and functional silos?  Agency  Type  Mature  IMP*   Pilot  IMP*  Experimental  Development  Planned  IMP*  No  Current  Plans  CFO Act  Agency*  2  4  1  3  2  Non-CFO  Act Agency  4  6  1  6  6  *IMP - Implementation  Figure 3. 2023 Federal Chief Data Officers Council Annual Survey response. Federal agency  implementation of data analytics across various components and functional silos.    0 1 2 3 4 5 6 7 Mature Implementation Pilot Implementation Experimental Development Planned Implementation No Current Plans Has your agency implemented an enterprise analytics  platform for integrating and analyzing data across various  components and funcitonal silos? Non-CFO Act Agency CFO Act Agency*\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of one extracted page of document\n",
    "document_1 = get_all_document()\n",
    "document_1[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29c33d-4c1f-47a4-9ecf-018f8efd6483",
   "metadata": {},
   "source": [
    "### 1.2. Document chunking<a id=\"1.2\"></a>\n",
    "#### 1.2.1. Create chunks of same length from sentences<a id=\"1.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f82add-8ebd-4104-9858-7cdd2097e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(content_list, nlp = NLP):\n",
    "    '''\n",
    "    Split page of content to sentences\n",
    "    '''\n",
    "    for page in tqdm(content_list):\n",
    "        # Process the text and create sentences\n",
    "        content = page['content']\n",
    "        doc = nlp(content) \n",
    "        sentences = [sentence.text for sentence in doc.sents]\n",
    "        # Create a new item in dictionary\n",
    "        page[\"sentences\"] = sentences\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca9302f-2cbd-435a-bfab-7b1f1cb7a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(content_list, s_1 = CHUNK_SIZE, s_2 = INTERSECT_SIZE):\n",
    "    '''\n",
    "    Create chunks from sentences\n",
    "    '''\n",
    "    page_len = len(content_list)\n",
    "    skip_num = 0\n",
    "    for i in tqdm(range(page_len)):\n",
    "        page = content_list[i]\n",
    "        sentences = page[\"sentences\"]\n",
    "        if skip_num < len(sentences):\n",
    "            start_i = max(0, skip_num-s_2)\n",
    "            chunks = [sentences[i:(i+s_1)] for i in range(start_i, \n",
    "                                                          len(sentences), \n",
    "                                                          (s_1-s_2))]\n",
    "        # Fill each chunk to the same length\n",
    "        if (len(chunks[-1]) < s_1) and (i != (page_len-1)):\n",
    "            skip_num = s_1 - len(chunks[-1])\n",
    "            next_page = content_list[i+1]\n",
    "            fill_sentences = next_page[\"sentences\"][:skip_num]\n",
    "            chunks[-1].extend(fill_sentences)\n",
    "        page[\"chunks\"] = chunks\n",
    "    return content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c84579c-8a73-4be1-99da-69bfd86b15d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7985de8215a46088fdda1795b1b5638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e34eddfc3b44e12a27e32e67bda28ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the chunk: 10.\n",
      "-------------------------\n",
      "['Non-CFO Act Agency CFO Act Agency*', '13    Effective collaboration, both within and between departments, is crucial.', \"The USDA's EDAPT  system and the State Department's Data.\", 'State both facilitated collaboration by integrating data  from various sources into a single platform, allowing for easier sharing and analysis.', 'The  partnership models between the CDO and CIO also played an integral role in the successful  implementation and management of these systems.  ', 'Given the lack of maturity in platform and data integration but the relatively higher maturity in  enterprise analytics programs, CDOs should leverage the interest in analytics to accelerate the  maturity of their overall data infrastructure to make the most of their analytics capabilities.  ', 'Facilitate rapid prototyping and proofs of concept on advanced analytics tools and capabilities  with innovative programs and customer agencies, while enabling the agency to learn and scale as  appropriate.  ', 'Both USDA and the Department of State recognized the need to develop advanced technical  capacities.', 'This was evident in the creation of sophisticated, enterprise-wide data platforms like  EDAPT and Data.', 'State, which enabled the consolidation, analysis, and visualization of data from  multiple sources.  ']\n"
     ]
    }
   ],
   "source": [
    "# Example of extracted sentences from one page of document\n",
    "document_2 = get_sentences(document_1)\n",
    "#document_2[11]['sentences']\n",
    "\n",
    "# Example of chunk of sentences of one page\n",
    "document_3 = get_chunks(document_2)\n",
    "example_chunk_len = len(document_3[11]['chunks'][-1])\n",
    "print(f'Number of sentences in the chunk: {example_chunk_len}.')\n",
    "print('-' * 25)\n",
    "print(document_3[11]['chunks'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdba2fda-ba44-4d0f-9d9b-a47c10344c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'page', 'content', 'sentences', 'chunks'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_3[11].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66cf431-e443-442d-96bc-be5103d62ab1",
   "metadata": {},
   "source": [
    "#### 1.2.2. Join chunk of sentences to a single string<a id=\"1.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa84629-f67c-4a3b-a1ec-700364a1f5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>page</th>\n",
       "      <th>chunk_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enterprise-Analytics-Report-508-FINAL.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>FEDERAL CHIEF DATA OFFICERS (CDO) COUNCIL The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Enterprise-Analytics-Report-508-FINAL.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>18 Appendix: Agency Case Studies ................</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enterprise-Analytics-Report-508-FINAL.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>They worked to build critical relationships an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       title  page  \\\n",
       "0  Enterprise-Analytics-Report-508-FINAL.pdf     1   \n",
       "1  Enterprise-Analytics-Report-508-FINAL.pdf     2   \n",
       "2  Enterprise-Analytics-Report-508-FINAL.pdf     3   \n",
       "\n",
       "                                        chunk_string  \n",
       "0  FEDERAL CHIEF DATA OFFICERS (CDO) COUNCIL The ...  \n",
       "1  18 Appendix: Agency Case Studies ................  \n",
       "2  They worked to build critical relationships an...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def join_chunk_sentences(chunk_list):\n",
    "    '''\n",
    "    Join the sentences in one chunk as a single string\n",
    "    '''\n",
    "    joined_chunk = \" \".join(chunk_list)\n",
    "    # Use re.sub to replace multiple spaces with a single space\n",
    "    chunk_string = re.sub(r'\\s+', ' ', joined_chunk).strip()\n",
    "    return chunk_string\n",
    "\n",
    "def get_chunk_df(content_list):\n",
    "    '''\n",
    "    Create a new content_list df based on generated chunks\n",
    "    '''\n",
    "    # Transform the original content_list to a pandas dataframe\n",
    "    raw_df = pd.DataFrame(content_list)\n",
    "    # Explode raw_df and create a new column of chunk string\n",
    "    new_df = raw_df.explode(['chunks'], ignore_index=True)\n",
    "    new_df['chunk_string'] = new_df['chunks'].apply(join_chunk_sentences)\n",
    "    # Select columns of interest\n",
    "    out_df = new_df[['title', 'page', 'chunk_string']]\n",
    "    return out_df\n",
    "\n",
    "chunk_df = get_chunk_df(document_3)\n",
    "chunk_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f868cde-81d1-4c6d-94ec-691e21679f05",
   "metadata": {},
   "source": [
    "### 1.3 Chunk embedding<a id=\"1.3\"></a>\n",
    "#### 1.3.1. Import pre-trained sentence-transformers model<a id=\"1.3.1\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44b3f6c-98f9-4ffe-be73-46b515b486d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c814772-1004-45b8-b248-3058eea6330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device)\n",
    "embedding_model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4049953e-8d79-41b1-a8f2-e1333400a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunk_df['chunk_embedding'] = chunk_df['chunk_string'].apply(lambda s: \n",
    "                                                             #embedding_model.encode(s))\n",
    "\n",
    "# Batch processing\n",
    "chunk_strings =  chunk_df['chunk_string'].tolist()\n",
    "chunk_embeddings = embedding_model.encode(chunk_strings,\n",
    "                                          batch_size=32, \n",
    "                                          convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c571287d-c5e7-439b-ae47-0ecb592f9e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1211, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d84fe8-00e1-4fdf-a9e6-8321d419abf1",
   "metadata": {},
   "source": [
    "#### 1.3.2. Save the domain knowledge with embeddings<a id=\"1.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbb564e-bfd6-4f49-9aec-aa08efd080a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_df.to_csv((DOMAIN_KNOWLEDGE_PATH+'domain_knowledge_chunks.csv'), encoding='utf-8', index=False)\n",
    "np.savetxt((DOMAIN_KNOWLEDGE_PATH+'domain_knowledge_embeddings.csv'), np.array(chunk_embeddings.cpu()), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2cca12-e065-4e9f-83bb-06cf26eeccf9",
   "metadata": {},
   "source": [
    "## 2. Prompt Augmentation<a id=\"2\"></a>\n",
    "### 2.1. Semantic search top k relevent chunks<a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9511d464-08ed-4295-aa61-1bcc9e21db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_chunks(query, chunk_embeddings, model, k=5):\n",
    "    '''\n",
    "    Retrieve top-k relevent chunks to input query\n",
    "    Return a list of chunk string\n",
    "    '''\n",
    "    # Embed the query with the same embedding model\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    # Get the similarity matrix with dot product\n",
    "    similarity_mat = util.dot_score(query_embedding, chunk_embeddings)[0]\n",
    "    # Get the top-k indices\n",
    "    top_indices = torch.topk(similarity_mat, k=k)[1].tolist()\n",
    "    # Get chunks\n",
    "    top_chunks = chunk_df.iloc[top_indices]['chunk_string'].tolist()\n",
    "    return top_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c66b1e7-8e87-4395-8bcc-7b994897ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(77) Guidance on the implementation of appropriate measures and on the demonstration of compliance by the controller or the processor, especially as regards the identification of the risk related to the processing, their assessment in terms of origin, nature, likelihood and severity, and the identification of best practices to mitigate the risk, could be provided in particular by means of approved codes of conduct, approved certifications, guidelines provided by the Board or indications provided by a data protection officer. The Board may also issue guidelines on processing operations that are considered to be unlikely to result in a high risk to the rights and freedoms of natural persons and indicate what measures may be sufficient in such cases to address such risk. (78) The protection of the rights and freedoms of natural persons with regard to the processing of personal data require that appropriate technical and organisational measures be taken to ensure that the requirements of this Regulation are met. In order to be able to demonstrate compliance with this Regulation, the controller should adopt internal policies and implement measures which meet in particular the principles of data protection by design and data protection by default. Such measures could consist, inter alia, of minimising the processing of personal data, pseudonymising personal data as soon as possible, transparency with regard to the functions and processing of personal data, enabling the data subject to monitor the data processing, enabling the controller to create and improve security features. When developing, designing, selecting and using applications, services and products that are based on the processing of personal data or process personal data to fulfil their task, producers of the products, services and applications should be encouraged to take into account the right to data protection when developing and designing such products, services and applications and, with due regard to the state of the art, to make sure that controllers and processors are able to fulfil their data protection obligations. The principles of data protection by design and by default should also be taken into consideration in the context of public tenders. (79) The protection of the rights and freedoms of data subjects as well as the responsibility and liability of controllers and processors, also in relation to the monitoring by and measures of supervisory authorities, requires a clear allocation of the responsibilities under this Regulation, including where a controller determines the purposes and means of the processing jointly with other controllers or where a processing operation is carried out on behalf of a controller. (80) Where a controller or a processor not established in the Union is processing personal data of data subjects who are in the Union whose processing activities are related to the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union, or to the monitoring of their behaviour as far as their behaviour takes place within the Union, the controller or the processor should designate a representative, unless the processing is occasional, does not include processing, on a large scale, of special categories of personal data or the processing of personal data relating to criminal convictions and offences, and is unlikely to result in a risk to the rights and freedoms of natural persons, taking into account the 4.5.2016 L 119/15 Official Journal of the European Union EN', 'In order to maintain security and to prevent processing in infringement of this Regulation, the controller or processor should evaluate the risks inherent in the processing and implement measures to mitigate those risks, such as encryption. Those measures should ensure an appropriate level of security, including confidentiality, taking into account the state of the art and the costs of implementation in relation to the risks and the nature of the personal data to be protected. In assessing data security risk, consideration should be given to the risks that are presented by personal data processing, such as accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed which may in particular lead to physical, material or non-material damage. (84) In order to enhance compliance with this Regulation where processing operations are likely to result in a high risk to the rights and freedoms of natural persons, the controller should be responsible for the carrying-out of a data protection impact assessment to evaluate, in particular, the origin, nature, particularity and severity of that risk. The outcome of the assessment should be taken into account when determining the appropriate measures to be taken in order to demonstrate that the processing of personal data complies with this Regulation. Where a data-protection impact assessment indicates that processing operations involve a high risk which the controller cannot mitigate by appropriate measures in terms of available technology and costs of implementation, a consultation of the supervisory authority should take place prior to the processing. (85) A personal data breach may, if not addressed in an appropriate and timely manner, result in physical, material or non-material damage to natural persons such as loss of control over their personal data or limitation of their rights, discrimination, identity theft or fraud, financial loss, unauthorised reversal of pseudonymisation, damage to reputation, loss of confidentiality of personal data protected by professional secrecy or any other significant economic or social disadvantage to the natural person concerned. Therefore, as soon as the controller becomes 4.5.2016 L 119/16 Official Journal of the European Union EN']\n"
     ]
    }
   ],
   "source": [
    "# Example: Get top 5 chunks related to the example query\n",
    "example_query = 'How to protect personal data?'\n",
    "top_chunks = get_top_chunks(example_query, chunk_embeddings, embedding_model)\n",
    "# Print top 2 related chunks\n",
    "print(top_chunks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792bdee-df49-462d-a7e3-45fc75bdd76b",
   "metadata": {},
   "source": [
    "## 3. Text Generation<a id=\"3\"></a>\n",
    "### 3.1. Import pre-trained seq2seq model<a id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28c22b4-77a0-4c2a-bdc0-1a717a3a4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_model_id = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355208b6-ac6e-479c-8e7a-d8c53a65b880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(generation_model_id, \n",
    "                                                       torch_dtype=torch.bfloat16, \n",
    "                                                       low_cpu_mem_usage=False)\n",
    "generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_id)\n",
    "generation_tokenizer.model_max_length=2048\n",
    "\n",
    "# Send the model to GPU\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e6ad1f-3bee-4220-bf29-17549987020a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e20c9-a017-4d52-8db8-52d9a94c4018",
   "metadata": {},
   "source": [
    "#### 3.2. Test the Model with Zero-Shot Inferencing<a id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "432a8164-892d-433f-a3a5-b97928e71194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_prompt(context, question):\n",
    "    '''\n",
    "    Prepend instruction to the question\n",
    "    '''\n",
    "    prepend_str = 'Based on the following context, please answer the query by extracting relevant passages from the context.\\nContext:\\n'\n",
    "    adj_question = prepend_str + context + '\\nQuestion:\\n' + question + '\\nAnswer:'\n",
    "    return adj_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e512bd4a-f82a-4c10-9ed0-3339dd2e6084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78) The protection of the rights and freedoms of natural persons with regard to the processing of personal data require that appropriate technical and organisational measures be taken to ensure that the requirements of this Regulation are met. In order to maintain security and to prevent processing in infringement of this Regulation, the controller should adopt internal policies and implement measures which meet in particular the principles of data protection by design and data protection by default. Such measures could consist, inter alia, of minimising the processing of personal data, pseudonymising personal data as soon as possible, transparency with regard to the functions and processing of personal data, enabling the data subject to monitor the data processing, enabling the controller to create and improve security features. When developing, designing, selecting and using applications, services and applications that are based on the processing of personal data or process personal data to fulfil their task, producers of the products, services and applications should be encouraged to take into account the right to data protection when developing and designing such products, services and applications and, with due regard to the state of the art, to make sure that controllers and processors are able to fulfil their data protection obligations. The principles of data protection by design and by default should also be taken\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot inference with example context and question\n",
    "example_context = '- ' + '\\n- '.join(top_chunks)\n",
    "example_prompt = prepend_prompt(example_context, example_query)\n",
    "#print(example_prompt)\n",
    "#print('-' * 100)\n",
    "\n",
    "example_inputs = generation_tokenizer(example_prompt, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "example_output = original_model.generate(example_inputs['input_ids'], max_new_tokens=256)[0]\n",
    "original_model_answer = generation_tokenizer.decode(example_output, skip_special_tokens=True)\n",
    "print(original_model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd110a-3b2c-450d-a9c7-89f5bb7530e1",
   "metadata": {},
   "source": [
    "## 4. Prompt Augmentation<a id=\"4\"></a>\n",
    "### 4.1. Fine-tune the generation model for abstractive question answering<a id=\"4.1\"></a>\n",
    "#### 4.1.1. Preprocess the abstractive-qa dataset<a id=\"4.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b825711f-ec7b-4971-ad60-14f417acec7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a60cb2ce594da6bafdd4ac218c8693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ae707f48a8477cbe240def386ef1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c57f107a084b179921ee97f24b4ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b2d9b75ce24ed997cbacdcb4ec71ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698808cb6c5040609806dc2c963d94d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_dataset = load_dataset('rajpurkar/squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e76b05b5-9f05-41c3-9099-1d297f3cf296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Query: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer: Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "# Example of qa_dataset's context and question-answer pair:\n",
    "dash_line = '-' * 100\n",
    "example_context = qa_dataset['train'][0]['context']\n",
    "example_question = qa_dataset['train'][0]['question']\n",
    "example_answers = qa_dataset['train'][0]['answers']['text'][0]\n",
    "print(f'Context: {example_context}')\n",
    "print(dash_line)\n",
    "print(f'Query: {example_question}')\n",
    "print(dash_line)\n",
    "print(f'Answer: {example_answers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f3d1e4d-8822-42b3-879a-ca91527d0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the question-answer pairs into explicit instructions for the generation model\n",
    "def tokenize(examples, tokenizer=generation_tokenizer, device=device, max_length=2048, answer_max_length=256):\n",
    "    '''\n",
    "    Concatentate the qa pairs with the predefined prompt \n",
    "    Tokenize with context, and define the tokenized answer as the label\n",
    "    '''\n",
    "    # Prepend the instruction to the context and question\n",
    "    prepend_str = 'Based on the following context, please answer the query by extracting relevant passages from the context.\\nContext:\\n'\n",
    "    adj_questions = [prepend_str + context + '\\nQuestion:\\n' + question + '\\nAnswer:'\n",
    "                     for (context, question) \n",
    "                     in zip(examples['context'], examples['question'])]\n",
    "    answers = [each['text'][0] for each in examples['answers']]\n",
    "    \n",
    "    # Tokenize the questions and answers\n",
    "    tokenized_inputs = tokenizer(adj_questions, padding=True, truncation=True, \n",
    "                                 max_length=max_length, return_tensors=\"pt\")\n",
    "    tokenized_labels = tokenizer(answers, padding=True, truncation=True, \n",
    "                                 max_length=answer_max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    # Ensure the labels are prepared for the model\n",
    "    labels = tokenized_labels['input_ids']\n",
    "    # Replace the token IDs for padding with -100\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    model_inputs = {\n",
    "        'input_ids': tokenized_inputs['input_ids'],\n",
    "        'attention_mask': tokenized_inputs['attention_mask'],\n",
    "        'labels': labels\n",
    "    }\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0860cb92-49f9-4802-88a2-e812d77f1b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4a83674065423c993d3b45125c191e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d554a3d94d946e38169eecc84f226f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_qa_dataset = qa_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a8273-6eba-4b23-95d4-56671434c6d7",
   "metadata": {},
   "source": [
    "#### 4.1.2. Setup the LoRA model for Fine-Tuning<a id=\"4.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "364ddd07-ccdb-4f1e-8af0-35d66fe605a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 251,116,800 || trainable%: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Add LoRA adapter layers/parameters to the original LLM to be trained:\n",
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "# Print the number of trainable model parameters in the LoRA model:\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f11134-dcde-450a-b646-43abd45fd1fe",
   "metadata": {},
   "source": [
    "#### 4.1.3. Train the LoRA Adapter<a id=\"4.1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac7e6ba0-dd48-4628-8416-cb548f8fcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_lora_qa\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-3, \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs_lora_qa',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Define data collator\n",
    "data_collator = DataCollatorForSeq2Seq(generation_tokenizer, model=peft_model)\n",
    "\n",
    "# Create a Seq2SeqTrainer instance\n",
    "peft_trainer = Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_qa_dataset['train'],\n",
    "    eval_dataset=tokenized_qa_dataset['validation'],\n",
    "    tokenizer=generation_tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d8ed779-8baa-447c-adc0-a7d0271cbce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32850' max='32850' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32850/32850 2:53:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.384900</td>\n",
       "      <td>0.322577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.331443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.311800</td>\n",
       "      <td>0.333329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.339710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.348100</td>\n",
       "      <td>0.333120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.349500</td>\n",
       "      <td>0.341716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>0.339754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.336479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.336209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.318322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.385400</td>\n",
       "      <td>0.315633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.321438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.316900</td>\n",
       "      <td>0.309952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.322916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.315259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.221500</td>\n",
       "      <td>0.309503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.270700</td>\n",
       "      <td>0.319164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.257500</td>\n",
       "      <td>0.319314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.309817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.305967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.428900</td>\n",
       "      <td>0.313904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.250100</td>\n",
       "      <td>0.316948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.402000</td>\n",
       "      <td>0.315961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.312596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.312317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.296100</td>\n",
       "      <td>0.313381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.307995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.362400</td>\n",
       "      <td>0.311012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.312031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.328900</td>\n",
       "      <td>0.315456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.332600</td>\n",
       "      <td>0.309948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>0.310418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.314059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.308331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>0.311278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>0.308165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.231100</td>\n",
       "      <td>0.308128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.309716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.279100</td>\n",
       "      <td>0.306877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>0.304898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.379800</td>\n",
       "      <td>0.301256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.305021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.276500</td>\n",
       "      <td>0.305486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.274600</td>\n",
       "      <td>0.303423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.341500</td>\n",
       "      <td>0.308143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.220100</td>\n",
       "      <td>0.308651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.308044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.243300</td>\n",
       "      <td>0.307384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.331500</td>\n",
       "      <td>0.306756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.265800</td>\n",
       "      <td>0.305761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.307696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.218900</td>\n",
       "      <td>0.307194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.306476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.221600</td>\n",
       "      <td>0.307226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.252500</td>\n",
       "      <td>0.305376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.304501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.304850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.274300</td>\n",
       "      <td>0.304823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.321200</td>\n",
       "      <td>0.304729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.304709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.325900</td>\n",
       "      <td>0.304695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.266000</td>\n",
       "      <td>0.304941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.304886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>0.304794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.373300</td>\n",
       "      <td>0.304929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32850, training_loss=0.3099739267474077, metrics={'train_runtime': 10391.7266, 'train_samples_per_second': 25.289, 'train_steps_per_second': 3.161, 'total_flos': 2.680364888277166e+17, 'train_loss': 0.3099739267474077, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d573a65-0e2d-405f-945a-11db06e3ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a local folder\n",
    "peft_model.save_pretrained('./flan-t5-base-qa-lora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2bdfd391-681b-4b11-becd-c01f82f90e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): lora.Linear(\n",
       "                (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained('./flan-t5-base-qa-lora')\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "peft_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e66a0590-7b55-49e1-a90c-eda6b74f025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a controller determines the purposes and means of the processing jointly with other controllers or where a processing operation is carried\n"
     ]
    }
   ],
   "source": [
    "# Zero-Shot inference with example context and question\n",
    "example_context = '- ' + '\\n- '.join(top_chunks)\n",
    "example_prompt = prepend_prompt(example_context, example_query)\n",
    "#print(example_prompt)\n",
    "#print('-' * 100)\n",
    "\n",
    "example_inputs = tokenizer(example_prompt, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "peft_output = peft_model.generate(example_inputs['input_ids'], max_new_tokens=256)[0]\n",
    "peft_model_answer = tokenizer.decode(peft_output, skip_special_tokens=True)\n",
    "print(peft_model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96975a90-de99-4552-b553-f5c5c2707c32",
   "metadata": {},
   "source": [
    "### 4.2. Prompt engineering a causal language model for abstractive question answering<a id=\"4.2\"></a>\n",
    "#### 4.2.1. Model selection and setup<a id=\"4.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60d4a1bc-21b1-4f5d-af1f-6d42fe3d5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_model_id = \"google/gemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c490d752-e688-454c-9777-cfeb1243ba28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1912b73138243268e01fa1335292747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=causal_model_id)\n",
    "causal_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=causal_model_id,\n",
    "                                                    torch_dtype=torch.bfloat16,\n",
    "                                                    low_cpu_mem_usage=False,\n",
    "                                                    attn_implementation='flash_attention_2')\n",
    "causal_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf8cae-2da9-43d8-b8a7-f0d80341948b",
   "metadata": {},
   "source": [
    "#### 4.2.2. Answer from causal model without prompt engineering<a id=\"4.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2268403-bab4-428d-b329-8a197f5a6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_prompt(raw_prompt, causal_tokenizer):\n",
    "    '''\n",
    "    Fill the prompt following huggingface chat model template\n",
    "    Return a tokenizable string the model expects\n",
    "    '''\n",
    "    template = [{\"role\": \"user\", \"content\": raw_prompt}]\n",
    "    chat_prompt = causal_tokenizer.apply_chat_template(conversation=template,\n",
    "                                                       tokenize=False, \n",
    "                                                       add_generation_prompt=True)\n",
    "    return chat_prompt\n",
    "\n",
    "def get_answer(chat_prompt, causal_tokenizer, causal_model, \n",
    "               device = device,\n",
    "               max_tokens = 256):\n",
    "    '''\n",
    "    Get formatted original answer given the prompt\n",
    "    '''\n",
    "    prompt_ids = causal_tokenizer(chat_prompt, return_tensors=\"pt\").to(device)\n",
    "    raw_answer_ids = causal_model.generate(**prompt_ids, max_new_tokens=max_tokens)\n",
    "    # Get the answer in original format\n",
    "    raw_answer = causal_tokenizer.decode(raw_answer_ids[0])\n",
    "    # Change the format of the answer\n",
    "    remove_list = [chat_prompt, '<bos>', '<eos>']\n",
    "    big_regex = re.compile('|'.join(map(re.escape, remove_list)))\n",
    "    answer = big_regex.sub('', raw_answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ff23ff0-fc28-455e-a5c2-5d2521cc126a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "How to protect personal data?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of fill the chat model prompt\n",
    "raw_prompt = 'How to protect personal data?'\n",
    "chat_prompt = get_chat_prompt(raw_prompt, causal_tokenizer)\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "977fc6d7-1361-4996-9a49-e6ea02d8625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Personal Data Protection Measures:**\n",
      "\n",
      "**1. Secure Data Collection and Storage:**\n",
      "\n",
      "* Use encrypted data collection forms and secure storage solutions (e.g., encrypted hard drives, cloud storage with strong security protocols).\n",
      "* Implement access controls to restrict unauthorized access to personal data.\n",
      "* Comply with industry standards for data protection (e.g., GDPR, CCPA).\n",
      "\n",
      "**2. Data Masking and Anonymization:**\n",
      "\n",
      "* Mask sensitive personal data (e.g., names, addresses, social security numbers) with anonymization techniques.\n",
      "* Use pseudonymization or data anonymization to remove identifying information from data.\n",
      "\n",
      "**3. Access Control and Authentication:**\n",
      "\n",
      "* Implement strong authentication methods (e.g., multi-factor authentication, biometric authentication).\n",
      "* Limit data access to authorized personnel only.\n",
      "* Use role-based access control (RBAC) to restrict data access based on user roles.\n",
      "\n",
      "**4. Data Retention and Deletion:**\n",
      "\n",
      "* Establish clear data retention policies to determine how long personal data should be stored.\n",
      "* Implement procedures for securely deleting personal data when it is no longer required.\n",
      "\n",
      "**5. Security Measures:**\n",
      "\n",
      "* Use firewalls, intrusion detection systems (IDS), and other security tools to protect against unauthorized access and\n"
     ]
    }
   ],
   "source": [
    "# Example of formatted answer w/o prompt engineering to the example prompt\n",
    "example_answer = get_answer(chat_prompt, causal_tokenizer, causal_model)\n",
    "print(example_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc22d7-6e9b-43f4-90d6-afa3bf67db22",
   "metadata": {},
   "source": [
    "#### 4.2.3. Answer from causal model with prompt engineering<a id=\"4.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0064eb62-3626-47bc-9471-06ee073dd422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instruction prompt with few-shot inference\n",
    "\n",
    "rag_prompt = '''Please answer the user's query with information from given context.\n",
    "Please extract relevant passages from the context before responding.\n",
    "Return the answer in an informative and concise manner. \n",
    "\\nUse the following examples as reference for the ideal answer style:\n",
    "\\nExample 1:\n",
    "Query: What is data risk?\n",
    "Answer: Data risk refers to the potential for data to be lost, stolen, corrupted, or otherwise compromised, leading to negative consequences for individuals or organizations. This risk encompasses a wide range of issues, including but not limited to data breaches, unauthorized access, data corruption, and privacy violations. The implications of data risk can be severe, impacting the financial health, reputation, and legal standing of organizations, as well as the privacy and security of individuals' personal information.\n",
    "\\nExample 2:\n",
    "Query: Where does data risk arise from?\n",
    "Answer: Data risk arises from various sources, including cyberattacks (such as hacking, phishing, and malware), human error (like accidental deletion or mishandling of data), technical failures (such as software bugs or hardware malfunctions), and natural disasters. The growing reliance on digital technology and the increasing volume of data being stored and processed have heightened the importance of managing data risks effectively.\n",
    "\\nExample 3:\n",
    "Query: How to conduct effective data risk management?\n",
    "Answer: Effective data risk management involves identifying potential risks, assessing their likelihood and potential impact, and implementing measures to mitigate these risks. This includes employing strong cybersecurity measures, developing and enforcing robust data handling policies, regular auditing and monitoring of data access and usage, and ensuring compliance with data protection regulations.\n",
    "\\nNow use the following context to answer user's query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "Query: {query}\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "def set_prompt(top_chunks, query, rag_prompt, tokenizer = causal_tokenizer):\n",
    "    '''\n",
    "    Use few-shot inference to conduct prompt engineering\n",
    "    '''\n",
    "    # Create the context from top-k chunks\n",
    "    context = \"- \" + \"\\n- \".join(top_chunks)\n",
    "    # Create the prompt with examples  \n",
    "    full_prompt = rag_prompt.format(context=context, query=query)\n",
    "    chat_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": full_prompt}\n",
    "    ]\n",
    "    out_prompt = tokenizer.apply_chat_template(conversation=chat_template,\n",
    "                                               tokenize=False,\n",
    "                                               add_generation_prompt=True)\n",
    "    return out_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bbcb034-f95a-4d91-b91b-cc4b267b7271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Please answer the user's query with information from given context.\n",
      "Please extract relevant passages from the context before responding.\n",
      "Return the answer in an informative and concise manner. \n",
      "\n",
      "Use the following examples as reference for the ideal answer style:\n",
      "\n",
      "Example 1:\n",
      "Query: What is data risk?\n",
      "Answer: Data risk refers to the potential for data to be lost, stolen, corrupted, or otherwise compromised, leading to negative consequences for individuals or organizations. This risk encompasses a wide range of issues, including but not limited to data breaches, unauthorized access, data corruption, and privacy violations. The implications of data risk can be severe, impacting the financial health, reputation, and legal standing of organizations, as well as the privacy and security of individuals' personal information.\n",
      "\n",
      "Example 2:\n",
      "Query: Where does data risk arise from?\n",
      "Answer: Data risk arises from various sources, including cyberattacks (such as hacking, phishing, and malware), human error (like accidental deletion or mishandling of data), technical failures (such as software bugs or hardware malfunctions), and natural disasters. The growing reliance on digital technology and the increasing volume of data being stored and processed have heightened the importance of managing data risks effectively.\n",
      "\n",
      "Example 3:\n",
      "Query: How to conduct effective data risk management?\n",
      "Answer: Effective data risk management involves identifying potential risks, assessing their likelihood and potential impact, and implementing measures to mitigate these risks. This includes employing strong cybersecurity measures, developing and enforcing robust data handling policies, regular auditing and monitoring of data access and usage, and ensuring compliance with data protection regulations.\n",
      "\n",
      "Now use the following context to answer user's query:\n",
      "- (77) Guidance on the implementation of appropriate measures and on the demonstration of compliance by the controller or the processor, especially as regards the identification of the risk related to the processing, their assessment in terms of origin, nature, likelihood and severity, and the identification of best practices to mitigate the risk, could be provided in particular by means of approved codes of conduct, approved certifications, guidelines provided by the Board or indications provided by a data protection officer. The Board may also issue guidelines on processing operations that are considered to be unlikely to result in a high risk to the rights and freedoms of natural persons and indicate what measures may be sufficient in such cases to address such risk. (78) The protection of the rights and freedoms of natural persons with regard to the processing of personal data require that appropriate technical and organisational measures be taken to ensure that the requirements of this Regulation are met. In order to be able to demonstrate compliance with this Regulation, the controller should adopt internal policies and implement measures which meet in particular the principles of data protection by design and data protection by default. Such measures could consist, inter alia, of minimising the processing of personal data, pseudonymising personal data as soon as possible, transparency with regard to the functions and processing of personal data, enabling the data subject to monitor the data processing, enabling the controller to create and improve security features. When developing, designing, selecting and using applications, services and products that are based on the processing of personal data or process personal data to fulfil their task, producers of the products, services and applications should be encouraged to take into account the right to data protection when developing and designing such products, services and applications and, with due regard to the state of the art, to make sure that controllers and processors are able to fulfil their data protection obligations. The principles of data protection by design and by default should also be taken into consideration in the context of public tenders. (79) The protection of the rights and freedoms of data subjects as well as the responsibility and liability of controllers and processors, also in relation to the monitoring by and measures of supervisory authorities, requires a clear allocation of the responsibilities under this Regulation, including where a controller determines the purposes and means of the processing jointly with other controllers or where a processing operation is carried out on behalf of a controller. (80) Where a controller or a processor not established in the Union is processing personal data of data subjects who are in the Union whose processing activities are related to the offering of goods or services, irrespective of whether a payment of the data subject is required, to such data subjects in the Union, or to the monitoring of their behaviour as far as their behaviour takes place within the Union, the controller or the processor should designate a representative, unless the processing is occasional, does not include processing, on a large scale, of special categories of personal data or the processing of personal data relating to criminal convictions and offences, and is unlikely to result in a risk to the rights and freedoms of natural persons, taking into account the 4.5.2016 L 119/15 Official Journal of the European Union EN\n",
      "- In order to maintain security and to prevent processing in infringement of this Regulation, the controller or processor should evaluate the risks inherent in the processing and implement measures to mitigate those risks, such as encryption. Those measures should ensure an appropriate level of security, including confidentiality, taking into account the state of the art and the costs of implementation in relation to the risks and the nature of the personal data to be protected. In assessing data security risk, consideration should be given to the risks that are presented by personal data processing, such as accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to, personal data transmitted, stored or otherwise processed which may in particular lead to physical, material or non-material damage. (84) In order to enhance compliance with this Regulation where processing operations are likely to result in a high risk to the rights and freedoms of natural persons, the controller should be responsible for the carrying-out of a data protection impact assessment to evaluate, in particular, the origin, nature, particularity and severity of that risk. The outcome of the assessment should be taken into account when determining the appropriate measures to be taken in order to demonstrate that the processing of personal data complies with this Regulation. Where a data-protection impact assessment indicates that processing operations involve a high risk which the controller cannot mitigate by appropriate measures in terms of available technology and costs of implementation, a consultation of the supervisory authority should take place prior to the processing. (85) A personal data breach may, if not addressed in an appropriate and timely manner, result in physical, material or non-material damage to natural persons such as loss of control over their personal data or limitation of their rights, discrimination, identity theft or fraud, financial loss, unauthorised reversal of pseudonymisation, damage to reputation, loss of confidentiality of personal data protected by professional secrecy or any other significant economic or social disadvantage to the natural person concerned. Therefore, as soon as the controller becomes 4.5.2016 L 119/16 Official Journal of the European Union EN\n",
      "- However, the further retention of the personal data should be lawful where it is necessary, for exercising the right of freedom of expression and information, for compliance with a legal obligation, for the performance of a task carried out in the public interest or in the exercise of official authority vested in the controller, on the grounds of public interest in the area of public health, for archiving purposes in the public interest, scientific or historical research purposes or statistical purposes, or for the establishment, exercise or defence of legal claims. (66) To strengthen the right to be forgotten in the online environment, the right to erasure should also be extended in such a way that a controller who has made the personal data public should be obliged to inform the controllers which are processing such personal data to erase any links to, or copies or replications of those personal data. In doing so, that controller should take reasonable steps, taking into account available technology and the means available to the controller, including technical measures, to inform the controllers which are processing the personal data of the data subject's request. (67) Methods by which to restrict the processing of personal data could include, inter alia, temporarily moving the selected data to another processing system, making the selected personal data unavailable to users, or temporarily removing published data from a website. In automated filing systems, the restriction of processing should in principle be ensured by technical means in such a manner that the personal data are not subject to further processing operations and cannot be changed. The fact that the processing of personal data is restricted should be clearly indicated in the system. (68) To further strengthen the control over his or her own data, where the processing of personal data is carried out by automated means, the data subject should also be allowed to receive personal data concerning him or her which he or she has provided to a controller in a structured, commonly used, machine-readable and interoperable format, and to transmit it to another controller. Data controllers should be encouraged to develop interoperable formats that enable data portability. That right should apply where the data subject provided the personal data on the basis of his or her consent or the processing is necessary for the performance of a contract. It should not apply where processing is based on a legal ground other than consent or contract.\n",
      "- Organizational leaders are encouraged to maintain up-to-date, comprehensive ethical standards regarding data use and staff are responsible for learning and applying agency guidance appropriately. 2 - Respect the public, individuals, and communities. Data activities have the overarching goal of benefiting the public good. Responsible use of data begins with careful consideration of its potential impacts. Data initiatives should include considerations for unique community and local contexts and have an identified and clear benefit to society. 3 - Respect privacy and confidentiality. Privacy and confidentiality should always be protected in a manner that respects the dignity, rights, and freedom of data subjects. In this context, privacy is the state of being free from unwarranted intrusion into the private life of individuals, and confidentiality is the state of ones information being free from inappropriate access and misuse. An essential objective of privacy and confidentiality protection is to minimize potential negative consequences through measures such as comprehensive risk assessments, disclosure avoidance, and upholding data governance standards. Data activities involving individual privacy should align with the Fair Information Practice Principles (FIPPs).\n",
      "- In that regard, the number of data subjects, the age of the data and any appropriate safeguards adopted should be taken into consideration. (63) A data subject should have the right of access to personal data which have been collected concerning him or her, and to exercise that right easily and at reasonable intervals, in order to be aware of, and verify, the lawfulness of the processing. This includes the right for data subjects to have access to data concerning their health, for example the data in their medical records containing information such as diagnoses, examination results, assessments by treating physicians and any treatment or interventions provided. Every data subject should therefore have the right to know and obtain communication in particular with regard to the purposes for which the personal data are processed, where possible the period for which the personal data are processed, the recipients of the personal data, the logic involved in any automatic personal data processing and, at least when based on profiling, the consequences of such processing. Where possible, the controller should be able to provide remote access to a secure system which would provide the data subject with direct access to his or her personal data. That right should not adversely affect the rights or freedoms of others, including trade secrets or intellectual property and in particular the copyright protecting the software. However, the result of those considerations should not be a refusal to provide all information to the data subject. Where the controller processes a large quantity of information concerning the data subject, the controller should be able to request that, before the information is delivered, the data subject specify the information or processing activities to which the request relates. (64) The controller should use all reasonable measures to verify the identity of a data subject who requests access, in particular in the context of online services and online identifiers.\n",
      "\n",
      "Relevant passages: <extract relevant passages from the context here>\n",
      "Query: How to protect personal data?\n",
      "Answer:<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the example prompt with few-shot inference\n",
    "example_prompt = set_prompt(top_chunks, \n",
    "                            example_query, \n",
    "                            rag_prompt)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da7f4aba-9752-4c38-93db-deb7777c1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the answer from the causal language model with few-shot inference\n",
    "\n",
    "def get_answer_w_few_shot(prompt, temperature=0.9, model=causal_model, tokenizer=causal_tokenizer, device=device):\n",
    "    '''\n",
    "    Return the answer from the model with engineered prompt\n",
    "    '''\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # Generate the answer tokens\n",
    "    out_tokens = model.generate(**input_ids,\n",
    "                                do_sample=True, \n",
    "                                max_new_tokens=512,\n",
    "                                temperature=temperature)\n",
    "    # Get the answer\n",
    "    raw_answer = tokenizer.decode(out_tokens[0])\n",
    "    # Change the format of the answer\n",
    "    remove_list = [prompt, '<bos>', '<eos>']\n",
    "    big_regex = re.compile('|'.join(map(re.escape, remove_list)))\n",
    "    answer = big_regex.sub('', raw_answer)\n",
    "    return answer\n",
    "    \n",
    "def get_pure_answers(raw_answer):\n",
    "    '''\n",
    "    Get answers without Relevent passages\n",
    "    '''\n",
    "    try:\n",
    "        start_index = raw_answer.index('**Answer:**')\n",
    "        return raw_answer[start_index+13:]\n",
    "    except:\n",
    "        try:\n",
    "            start_index = raw_answer.index('**Conclusion:**')\n",
    "            return raw_answer[start_index+17:]\n",
    "        except:\n",
    "            return raw_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c2168d5-351b-4675-a90b-98539cc0f947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Relevant passages:**\n",
      "\n",
      "**1. Data risk:**\n",
      "- \"Data risk refers to the potential for data to be lost, stolen, corrupted, or otherwise compromised, leading to negative consequences for individuals or organizations.\"\n",
      "- \"The protection of the rights and freedoms of natural persons with regard to the processing of personal data require that appropriate technical and organisational measures be taken to ensure that the requirements of this Regulation are met.\"\n",
      "\n",
      "**2. Data protection measures:**\n",
      "- \"In order to maintain security and to prevent processing in infringement of this Regulation, the controller or processor should evaluate the risks inherent in the processing and implement measures to mitigate those risks.\"\n",
      "- \"To further strengthen the control over his or her own data, where the processing of personal data is carried out by automated means, the data subject should also be allowed to receive personal data concerning him or her which he or she has provided to a controller in a structured, commonly used, machine-readable and interoperable format, and to transmit it to another controller.\"\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "To protect personal data, appropriate technical and organizational measures must be taken to ensure compliance with the European Union Regulation on the Protection of Personal Data (GDPR). These measures include evaluating the risks inherent in the processing, implementing measures to mitigate risks, and ensuring data protection by design and by default. Additionally, data subjects have the right to access, rectify, erase, or restrict the processing of their personal data. To enforce these rights, controllers should provide clear and concise information about the purpose, duration, and recipients of personal data processing. Measures to verify the identity of data subjects and prevent unauthorized access to personal data should also be implemented.\n"
     ]
    }
   ],
   "source": [
    "# Print the example answer with few-shot inference\n",
    "example_answer = get_answer_w_few_shot(example_prompt)\n",
    "print(example_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9543577-b7e8-4337-b615-cb06e2610644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To protect personal data, appropriate technical and organizational measures must be taken to ensure compliance with the European Union Regulation on the Protection of Personal Data (GDPR). These measures include evaluating the risks inherent in the processing, implementing measures to mitigate risks, and ensuring data protection by design and by default. Additionally, data subjects have the right to access, rectify, erase, or restrict the processing of their personal data. To enforce these rights, controllers should provide clear and concise information about the purpose, duration, and recipients of personal data processing. Measures to verify the identity of data subjects and prevent unauthorized access to personal data should also be implemented.\n"
     ]
    }
   ],
   "source": [
    "pure_example_answer = get_pure_answers(example_answer)\n",
    "print(pure_example_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e31b2-6c9f-4a5b-a8aa-2007671b7cfc",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation<a id=\"5\"></a>\n",
    "### 5.1. Evaluate the Model Quantitatively (with ROUGE Metric)<a id=\"5.1\"></a>\n",
    "#### 5.1.1. Create sample questions and human_baseline answers<a id=\"5.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4df5df9b-3806-4df5-b7bb-423147cab5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = [\n",
    "    'What is data governance?',\n",
    "    'When to establish a data governance body?',\n",
    "    'Who is authorized to establish a data governance body?',\n",
    "    'What is data identification as one key activity of data governance?',\n",
    "    'What is data management policy as one key activity of data governance?',\n",
    "    'What is joint controllers?',\n",
    "    'How to handle cybersecurity risk for an organization?',\n",
    "    'What is CSF Core?',\n",
    "    'When should agencies consider a mixed-mode approach for survey data collection?',\n",
    "    'How to enhance transparency and compliance with the regulation of GDPR?',\n",
    "    'What are OMB statistical classifications?'\n",
    "]\n",
    "\n",
    "human_answers = [\n",
    "    'Data governance is the process of setting and enforcing priorities for managing and using data as a strategic asset. A data governance body with authority and oversight over the management of agency data assets is a key piece of data infrastructure. These bodies are commonly called by such names as Data Governance Boards, Data Councils, or Data Strategy Teams. The data governance body establishes policies, procedures, and roles for developing, overseeing, and coordinating data management policy and helps prioritize data resource allocations to answer agency key questions and meet stakeholder needs.',\n",
    "    'Agencies should make establishing a data governance body a top priority, thereby setting up the organizational structure to address data and related infrastructure needs.',\n",
    "    'A data governance body is authorized and chartered by the agency head or delegated authority, chaired by the Chief Data Officer (CDO), and includes senior staff with responsibility for diverse aspects of data management as well as senior officials from agency program areas. In addition to the CDO, membership should include the Evaluation Official (EO) and the Statistical Official (SO) named in accordance with the Evidence Act.',\n",
    "    'Identify data assets and develop a data inventory with appropriate metadata.',\n",
    "    \"Develop short statements of management intent and fundamental rules for governing the creation, acquisition, privacy, integrity, security, quality, and use of data and information.\",\n",
    "    \"Where two or more controllers jointly determine the purposes and means of processing, they shall be joint controllers.\",\n",
    "    \"An organization may choose to handle risk in one or more ways  including mitigating, transferring, avoiding, or accepting negative risks and realizing, sharing, enhancing, or accepting positive risks  depending on the potential impacts and likelihoods. Importantly, an organization can use the CSF both internally to manage its cybersecurity capabilities and externally to oversee or communicate with third parties.\",\n",
    "    \"CSF Core, the nucleus of the CSF, which is a taxonomy of high-level cybersecurity outcomes that can help any organization manage its cybersecurity risks. The CSF Core components are a hierarchy of Functions, Categories, and Subcategories that detail each outcome.\",\n",
    "    \"The two main reasons to consider using more than one mode of collection simultaneously are cost and response rates. The typical mixed mode approach is to use a less costly method for initial contact and a more costly mode for follow-up with nonrespondents, such as using a mail survey with telephone nonresponse follow-up or a telephone survey with an in-person nonresponse follow-up.\",\n",
    "    \"In order to enhance transparency and compliance with this Regulation, the establishment of certification mechanisms and data protection seals and marks should be encouraged, allowing data subjects to quickly assess the level of data protection of relevant products and services.\",\n",
    "    \"OMB currently has a number of different statistical classifications for demographic, economic, and geographic data, including data on race and ethnicity, industries, occupations, and statistical areas described in more detail in the following questions. In addition, there are some standard definitions of economic concepts for statistical purposes, and standard sources for Federal data for some demographic and economic statistics.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722e0a7-29ca-41a1-bfd4-384d7cf8eafe",
   "metadata": {},
   "source": [
    "#### 5.1.2. Get answers from the causal language model<a id=\"5.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03b8b61c-27e8-4a36-85df-fd0d4ba51845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_causal_answer(query, chunk_embeddings=chunk_embeddings, embedding_model=embedding_model, rag_prompt=rag_prompt):\n",
    "    '''\n",
    "    Return the output from prompt engineered causal language model\n",
    "    '''\n",
    "    top_chunks = get_top_chunks(query, chunk_embeddings, embedding_model)\n",
    "    prompt = set_prompt(top_chunks, query, rag_prompt)\n",
    "    raw_answer = get_answer_w_few_shot(prompt)\n",
    "    return get_pure_answers(raw_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66e9626a-3d51-4db1-9353-3aa3938130f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_answers = [get_causal_answer(each) for each in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4679de3f-541f-490f-b2de-48407a577948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data management policy is one key activity of data governance as stated in the text. It is the development of short statements of management intent and fundamental rules for governing the creation, acquisition, privacy, integrity, security, quality, and use of data and information.\n"
     ]
    }
   ],
   "source": [
    "# Check causal answers format\n",
    "print(causal_answers[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2113f-0237-474e-8a56-82547b94df73",
   "metadata": {},
   "source": [
    "#### 5.1.3. Get answers from fine-tuned seq2seq language model<a id=\"5.1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3f8c550a-ebbf-43f5-b43c-9f1f2510b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq2seq_answer(query, chunk_embeddings=chunk_embeddings, embedding_model=embedding_model, tokenizer=tokenizer, peft_model=peft_model, device=device):\n",
    "    top_chunks = get_top_chunks(query, chunk_embeddings, embedding_model)\n",
    "    context = '- ' + '\\n- '.join(top_chunks)\n",
    "    prompt = prepend_prompt(context, query)\n",
    "    # Get the input for seq2seq model\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "    # Get the answer from seq2seq model\n",
    "    peft_output = peft_model.generate(inputs['input_ids'], max_new_tokens=512)[0]\n",
    "    answer = tokenizer.decode(peft_output, skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ffc3715e-a8ba-479f-9c87-05208c2cdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_answers = [get_seq2seq_answer(each) for each in question_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77254d76-a918-4bcb-be82-a05ccf85fde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Governance Boards, Data Councils, or Data Strategy Teams\n"
     ]
    }
   ],
   "source": [
    "# Check seq2seq answers format\n",
    "print(seq2seq_answers[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09968794-baad-4b4a-965d-692d766592c2",
   "metadata": {},
   "source": [
    "### 5.2. Import the ROUGE and BLEU metric<a id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9a3c38c6-c09b-4c66-91cf-2e0b7ad5cfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b5883-b223-4d4d-a4e6-0ca3f4d66883",
   "metadata": {},
   "source": [
    "### 5.3. Evaluate models performance<a id=\"5.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0cd74f1e-005b-4428-a76b-08a66f21bd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq model:\n",
      "{'rouge1': 0.15428224385214537, 'rouge2': 0.02203652537232967, 'rougeL': 0.12614576381579035, 'rougeLsum': 0.12585373876963152}\n",
      "causal model:\n",
      "{'rouge1': 0.3617099989759032, 'rouge2': 0.2523914257083444, 'rougeL': 0.3010443648148077, 'rougeLsum': 0.3145119502546009}\n"
     ]
    }
   ],
   "source": [
    "# ROUGE meric\n",
    "seq2seq_model_results = rouge.compute(\n",
    "    predictions=seq2seq_answers,\n",
    "    references=human_answers\n",
    ")\n",
    "\n",
    "causal_model_results = rouge.compute(\n",
    "    predictions=causal_answers,\n",
    "    references=human_answers\n",
    ")\n",
    "\n",
    "print('seq2seq model:')\n",
    "print(seq2seq_model_results)\n",
    "print('causal model:')\n",
    "print(causal_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "304d256c-37f4-472d-8c5b-02c7c73cb8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seq model:\n",
      "{'bleu': 0.00956830823474908, 'precisions': [0.3312883435582822, 0.09868421052631579, 0.07092198581560284, 0.06153846153846154], 'brevity_penalty': 0.08754670802118468, 'length_ratio': 0.2910714285714286, 'translation_length': 163, 'reference_length': 560}\n",
      "causal model:\n",
      "{'bleu': 0.1018484734792892, 'precisions': [0.17144373673036092, 0.0998398291510945, 0.08431793770139635, 0.07455429497568881], 'brevity_penalty': 1.0, 'length_ratio': 3.3642857142857143, 'translation_length': 1884, 'reference_length': 560}\n"
     ]
    }
   ],
   "source": [
    "# BLEU metric\n",
    "seq2seq_model_results_1 = bleu.compute(\n",
    "    predictions=seq2seq_answers,\n",
    "    references=human_answers\n",
    ")\n",
    "\n",
    "causal_model_results_1 = bleu.compute(\n",
    "    predictions=causal_answers,\n",
    "    references=human_answers\n",
    ")\n",
    "\n",
    "print('seq2seq model:')\n",
    "print(seq2seq_model_results_1)\n",
    "print('causal model:')\n",
    "print(causal_model_results_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
